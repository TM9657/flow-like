# =============================================================================
# Flow-Like Docker Compose Backend - Environment Configuration
# =============================================================================
# Copy this file to .env and customize for your environment

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
POSTGRES_USER=flowlike
POSTGRES_PASSWORD=flowlike_dev_change_me
POSTGRES_DB=flowlike
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Full DATABASE_URL (auto-constructed in docker-compose)
# DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

# -----------------------------------------------------------------------------
# Redis Configuration (Execution State + Job Queue)
# -----------------------------------------------------------------------------
REDIS_HOST=redis
REDIS_PORT=6379
# Full REDIS_URL (auto-constructed in docker-compose)
# REDIS_URL=redis://${REDIS_HOST}:${REDIS_PORT}

# Execution state backend: postgres, redis
# Redis is recommended for Docker Compose (native TTL, fast polling)
EXECUTION_STATE_BACKEND=redis
# TTL for execution state entries (default: 24 hours)
EXECUTION_STATE_TTL_SECONDS=86400

# -----------------------------------------------------------------------------
# Storage Provider Configuration
# -----------------------------------------------------------------------------
# Options: aws, azure, gcp
STORAGE_PROVIDER=aws

# Bucket/Container names (generic - works across all providers)
META_BUCKET=flow-like-meta
CONTENT_BUCKET=flow-like-content
LOG_BUCKET=flow-like-logs

# -----------------------------------------------------------------------------
# AWS S3 Configuration (when STORAGE_PROVIDER=aws)
# -----------------------------------------------------------------------------
# Authentication options:
#
# Option 1: Static credentials (for local dev, MinIO, R2)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
# Optional: session token (for temporary credentials from STS AssumeRole)
# AWS_SESSION_TOKEN=
#
# Option 2: IAM Instance Role (on EC2/ECS - automatic, no env vars needed)
# Option 3: IRSA/Web Identity (on EKS - set AWS_WEB_IDENTITY_TOKEN_FILE + AWS_ROLE_ARN)
# AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token
# AWS_ROLE_ARN=arn:aws:iam::123456789012:role/FlowLikeRole
#
# Endpoint configuration
# For AWS S3: leave empty to use defaults
# For Cloudflare R2: https://<account-id>.r2.cloudflarestorage.com
# For MinIO: http://minio:9000
AWS_ENDPOINT=
AWS_REGION=us-east-1
# Set to true for MinIO, R2, or other S3-compatible stores
AWS_USE_PATH_STYLE=true

# Role ARN for scoped credentials (required for per-user/per-app isolated S3 prefixes)
# This role will be assumed via STS to generate short-lived scoped credentials
# Example: arn:aws:iam::123456789012:role/FlowLikeRuntimeRole
RUNTIME_ROLE_ARN=

# Provider-specific bucket overrides (optional)
# AWS_META_BUCKET=
# AWS_CONTENT_BUCKET=
# AWS_LOG_BUCKET=

# -----------------------------------------------------------------------------
# Azure Blob Storage Configuration (when STORAGE_PROVIDER=azure)
# -----------------------------------------------------------------------------
AZURE_STORAGE_ACCOUNT_NAME=
AZURE_STORAGE_ACCOUNT_KEY=

# Provider-specific container overrides (optional)
# AZURE_META_CONTAINER=
# AZURE_CONTENT_CONTAINER=
# AZURE_LOG_CONTAINER=

# -----------------------------------------------------------------------------
# GCP Cloud Storage Configuration (when STORAGE_PROVIDER=gcp)
# -----------------------------------------------------------------------------
GCP_PROJECT_ID=
# Base64-encoded or raw service account JSON
GOOGLE_APPLICATION_CREDENTIALS_JSON=

# Provider-specific bucket overrides (optional)
# GCP_META_BUCKET=
# GCP_CONTENT_BUCKET=
# GCP_LOG_BUCKET=

# -----------------------------------------------------------------------------
# API Service Configuration
# -----------------------------------------------------------------------------
API_HOST=0.0.0.0
API_PORT=8080
RUST_LOG=info,docker_compose_api=debug

# -----------------------------------------------------------------------------
# Web Application Configuration
# -----------------------------------------------------------------------------
WEB_PORT=3001
# URL the web app uses to connect to the API (from browser perspective)
NEXT_PUBLIC_API_URL=http://localhost:8080
# OAuth redirect URLs (adjust domain/port for your environment)
NEXT_PUBLIC_REDIRECT_URL=http://localhost:3001/callback
NEXT_PUBLIC_REDIRECT_LOGOUT_URL=http://localhost:3001/

# -----------------------------------------------------------------------------
# Runtime Execution Configuration
# -----------------------------------------------------------------------------
RUNTIME_HOST=0.0.0.0
RUNTIME_PORT=9000

# Maximum concurrent executions per runtime instance
MAX_CONCURRENT_EXECUTIONS=10
# Execution timeout in seconds
EXECUTION_TIMEOUT_SECONDS=3600

# -----------------------------------------------------------------------------
# Execution Dispatch Configuration
# -----------------------------------------------------------------------------
# Backend for dispatching executions: http, redis
#   http  - Synchronous HTTP calls to EXECUTOR_URL (simpler, lower latency)
#   redis - Async queue via Redis LPUSH (better for streaming, retry support)
EXECUTION_BACKEND=redis

# HTTP executor URL (when EXECUTION_BACKEND=http)
EXECUTOR_URL=http://runtime:9000

# Redis queue name (when EXECUTION_BACKEND=redis)
REDIS_EXECUTION_QUEUE=exec:jobs

# -----------------------------------------------------------------------------
# Queue Worker Configuration (Runtime Service)
# -----------------------------------------------------------------------------
# Enable queue worker mode (polls Redis for jobs)
# When true, the runtime will poll the Redis queue in addition to serving HTTP
QUEUE_WORKER_ENABLED=true

# Concurrency for queue worker (independent of MAX_CONCURRENT_EXECUTIONS)
# Defaults to MAX_CONCURRENT_EXECUTIONS if not set
QUEUE_WORKER_CONCURRENCY=10

# Poll timeout for BRPOP in seconds (0 = block indefinitely)
QUEUE_POLL_TIMEOUT_SECS=30

# -----------------------------------------------------------------------------
# Backend JWT Configuration (REQUIRED)
# -----------------------------------------------------------------------------
# Generate these using: ../../tools/gen-execution-keys.sh
#
# These keys enable stateless trust between API and all backend services.
# A single keypair is used for all token types: executor, user polling, realtime.
# The token type is embedded in the JWT claims (typ field).
#
# IMPORTANT: Only the API needs the private key (BACKEND_KEY).
# Executors only need the public key (BACKEND_PUB) or can fetch it from the API.
#
# BACKEND_KEY: Base64-encoded PEM private key (ES256) - API only
# BACKEND_PUB: Base64-encoded PEM public key (ES256) - Optional for executors
# BACKEND_KID: Key identifier for JWKS

BACKEND_KEY=
BACKEND_PUB=
BACKEND_KID=backend-es256-v1

# API URL for executors to fetch JWKS if BACKEND_PUB is not set
# This allows zero-config secure JWT verification for executors
API_URL=http://api:8080

# -----------------------------------------------------------------------------
# LLM / Model Provider Configuration
# -----------------------------------------------------------------------------
# The backend can proxy LLM requests through various hosted providers.
# Models are defined as "Bits" in the database, each referencing a provider.
#
# Supported providers (set in Bit's provider_name field):
#   - hosted / hosted:openrouter  → Routes to OpenRouter
#   - hosted:openai               → Routes to OpenAI
#   - hosted:anthropic            → Routes to Anthropic
#   - hosted:azure                → Routes to Azure OpenAI
#   - hosted:bedrock              → Routes to AWS Bedrock
#   - hosted:vertex               → Routes to Google Vertex AI

# OpenRouter (default for "hosted" provider)
OPENROUTER_API_KEY=
OPENROUTER_ENDPOINT=https://openrouter.ai/api

# OpenAI
HOSTED_OPENAI_API_KEY=
HOSTED_OPENAI_ENDPOINT=https://api.openai.com

# Anthropic
HOSTED_ANTHROPIC_API_KEY=
HOSTED_ANTHROPIC_ENDPOINT=https://api.anthropic.com

# Azure OpenAI (endpoint includes your resource name)
HOSTED_AZURE_API_KEY=
HOSTED_AZURE_ENDPOINT=

# AWS Bedrock (uses AWS credentials from storage config above)
HOSTED_BEDROCK_ENDPOINT=

# Google Vertex AI
HOSTED_VERTEX_API_KEY=
HOSTED_VERTEX_ENDPOINT=

# -----------------------------------------------------------------------------
# Optional: External Services
# -----------------------------------------------------------------------------
# Sentry for error tracking
# SENTRY_DSN=https://...@sentry.io/...

# -----------------------------------------------------------------------------
# Monitoring (Optional)
# -----------------------------------------------------------------------------
METRICS_ENABLED=true
METRICS_PORT=9090

# Health check endpoints
HEALTH_CHECK_ENABLED=true

# -----------------------------------------------------------------------------
# Distributed Tracing (Optional - requires monitoring profile)
# -----------------------------------------------------------------------------
# OTLP endpoint for sending traces (Tempo in docker-compose)
OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4317
OTEL_EXPORTER_OTLP_PROTOCOL=grpc

# Sampling configuration
# parentbased_traceidratio samples based on parent trace or ratio
OTEL_TRACES_SAMPLER=parentbased_traceidratio
# Sample 10% of traces (0.1 = 10%, 1.0 = 100%)
OTEL_TRACES_SAMPLER_ARG=0.1

# Tempo ports (when using monitoring profile)
TEMPO_HTTP_PORT=3200
TEMPO_OTLP_GRPC_PORT=4317
TEMPO_OTLP_HTTP_PORT=4318
